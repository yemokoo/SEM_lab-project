# -*- coding: utf-8 -*-
import os
import time
import pandas as pd
from pyspark.sql import SparkSession, Window
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
from pyspark.sql import functions as F
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
import geopandas as gpd
import folium
import seaborn as sns
import matplotlib.pyplot as plt
from branca.colormap import LinearColormap
import itertools

# --- 1. ê²½ë¡œ ë° íŒŒë¼ë¯¸í„° ì„¤ì • ---
print("--- 1. ê²½ë¡œ ë° íŒŒë¼ë¯¸í„° ì„¤ì • ---")

PARQUET_BASE_PATH = r"D:\ì—°êµ¬ì‹¤\ì—°êµ¬\í™”ë¬¼ì°¨ ì¶©ì „ì†Œ ë°°ì¹˜ ìµœì í™”\Data\Processed_Data\simulator\Trajectory(MONTH_FULL)"
SHAPEFILE_PATH = r"D:\ì—°êµ¬ì‹¤\ì—°êµ¬\í™”ë¬¼ì°¨ ì¶©ì „ì†Œ ë°°ì¹˜ ìµœì í™”\Data\Raw_Data\main_road_network_level_5.5\level5_5_link_probe_32_2020.shp"
BASE_OUTPUT_DIR = r"D:\ì—°êµ¬ì‹¤\ì—°êµ¬\í™”ë¬¼ì°¨ ì¶©ì „ì†Œ ë°°ì¹˜ ìµœì í™”\Data\Data_Analysis"
STATISTICS_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, "Statistics")
VISUALIZATION_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, "Visualization")
YEARLY_LINK_VOLUME_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "link_yearly_volume_2020")
DAILY_TOTAL_VOLUME_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "daily_total_volume_2020")
CLUSTERED_STOPS_PARQUET_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "clustered_stops.parquet")
HEATMAP_HTML = os.path.join(VISUALIZATION_OUTPUT_DIR, "figure1_traffic_heatmap_ALL_LINKS.html")
BOXPLOT_PNG = os.path.join(VISUALIZATION_OUTPUT_DIR, "figure2_daily_volume_boxplot.png")
SCATTER_PLOT_PNG = os.path.join(VISUALIZATION_OUTPUT_DIR, "figure3_stop_duration_scatter.png")
TARGET_CRS = "EPSG:4326"

os.makedirs(STATISTICS_OUTPUT_DIR, exist_ok=True)
os.makedirs(VISUALIZATION_OUTPUT_DIR, exist_ok=True)

try:
    plt.rc('font', family='Malgun Gothic')
    plt.rc('axes', unicode_minus=False)
except Exception as e:
    print(f"ê²½ê³ : í•œê¸€ í°íŠ¸('Malgun Gothic') ì„¤ì • ì‹¤íŒ¨. ì˜¤ë¥˜: {e}")

processed_schema = StructType([
    StructField("OBU_ID", IntegerType(), True),
    StructField("DATETIME", StringType(), True),
    StructField("LINK_ID", IntegerType(), True),
    StructField("LINK_LENGTH", DoubleType(), True),
    StructField("DRIVING_TIME_MINUTES", DoubleType(), True),
    StructField("STOPPING_TIME", DoubleType(), True),
    StructField("CUMULATIVE_DRIVING_TIME_MINUTES", DoubleType(), True),
    StructField("CUMULATIVE_STOPPING_TIME_MINUTES", DoubleType(), True),
    StructField("CUMULATIVE_LINK_LENGTH", DoubleType(), True),
    StructField("SIGUNGU_ID", StringType(), True),
    StructField("AREA_ID", IntegerType(), True),
    StructField("START_TIME_MINUTES", DoubleType(), True),
])


# --- 2. Sparkì„ ì´ìš©í•œ ë°ì´í„° ë¶„ì„ (ì›”ë³„ ìˆœì°¨ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ì „ë©´ ìˆ˜ì •) ---
def run_spark_analysis():
    """Sparkì„ ì‚¬ìš©í•˜ì—¬ ì—°ê°„/ì¼ë³„ í†µí–‰ëŸ‰ ë° ì •ì§€ ìœ í˜• ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("\n--- 2. Spark ë¶„ì„ ì‹œì‘ (ì›”ë³„ ìˆœì°¨ ì²˜ë¦¬) ---")
    
    spark = (
        SparkSession.builder.appName("CombinedTrafficAnalysis_Monthly")
    .config("spark.executor.memory", "24g")
    .config("spark.driver.memory", "16g")  # Pandas ë³€í™˜ì„ ìœ„í•´ ë“œë¼ì´ë²„ ë©”ëª¨ë¦¬ ì¦ëŸ‰
    .config("spark.sql.shuffle.partitions", "800")
    .config("spark.default.parallelism", "400")
    .config("spark.memory.fraction", "0.8")
    .config("spark.executor.cores", "4")
    .config("spark.python.worker.reuse", "true")
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") # toPandas ì„±ëŠ¥ í–¥ìƒ
    .config("spark.sql.legacy.timeParserPolicy", "LEGACY")
    .getOrCreate()
    )
    
    # ìµœì¢… ê²°ê³¼ë¥¼ ëˆ„ì í•  ë¹„ì–´ìˆëŠ” ë°ì´í„°í”„ë ˆì„ ì´ˆê¸°í™”
    yearly_link_volume_acc = None
    daily_total_volume_acc = None
    stopping_df_acc = None
    
    # 1ì›”ë¶€í„° 12ì›”ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬
    for month in range(1, 13):
        year = 2020
        month_str = f"{year}-{month:02d}"
        monthly_path = os.path.join(PARQUET_BASE_PATH, month_str)
        
        print(f"\n--- {month_str} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ ---")
        
        try:
            # í•´ë‹¹ ì›”ì˜ ë°ì´í„°ë§Œ ì½ê¸°
            df_month = spark.read.schema(processed_schema).parquet(monthly_path)
            # ì›”ë³„ ë°ì´í„° ìºì‹± (ë©”ëª¨ë¦¬ ë¶€ë‹´ì´ ì ìŒ)
            df_month.cache()
            
            count = df_month.count()
            if count == 0:
                print(f"  - {month_str} í´ë”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                df_month.unpersist()
                continue
            
            print(f"  - {month_str} ë°ì´í„° {count:,} ê±´ ë¡œë“œ ì™„ë£Œ.")

            # 2a. ì›”ê°„ ë§í¬ë³„ í†µí–‰ëŸ‰ ì§‘ê³„ ë° ëˆ„ì 
            monthly_link_volume = df_month.groupBy("LINK_ID").count()
            if yearly_link_volume_acc is None:
                yearly_link_volume_acc = monthly_link_volume
            else:
                # full_outer joinì„ í†µí•´ ë‘ ë°ì´í„°í”„ë ˆì„ì˜ countë¥¼ í•©ì‚°
                yearly_link_volume_acc = yearly_link_volume_acc.join(monthly_link_volume, "LINK_ID", "full_outer") \
                    .select(
                        F.col("LINK_ID"),
                        (F.coalesce(yearly_link_volume_acc["count"], F.lit(0)) + F.coalesce(monthly_link_volume["count"], F.lit(0))).alias("count")
                    )
            print(f"  - {month_str} ë§í¬ í†µí–‰ëŸ‰ ëˆ„ì  ì™„ë£Œ.")

            # 2b. ì›”ê°„ ì¼ë³„ ì´ í†µí–‰ëŸ‰ ì§‘ê³„ ë° ëˆ„ì 
            monthly_daily_volume = df_month.withColumn("date", F.to_date("DATETIME")).groupBy("date").count().withColumnRenamed("count", "Daily_Volume")
            if daily_total_volume_acc is None:
                daily_total_volume_acc = monthly_daily_volume
            else:
                daily_total_volume_acc = daily_total_volume_acc.unionByName(monthly_daily_volume)
            print(f"  - {month_str} ì¼ë³„ í†µí–‰ëŸ‰ ëˆ„ì  ì™„ë£Œ.")

            # 2c. ì›”ê°„ ì •ì§€ ì‹œê°„ ë°ì´í„° ëˆ„ì 
            monthly_stopping_df = df_month.filter(F.col("STOPPING_TIME") > 0)
            if stopping_df_acc is None:
                stopping_df_acc = monthly_stopping_df
            else:
                stopping_df_acc = stopping_df_acc.unionByName(monthly_stopping_df)
            print(f"  - {month_str} ì •ì§€ ë°ì´í„° ëˆ„ì  ì™„ë£Œ.")
            
            df_month.unpersist() # ì›”ë³„ ìºì‹œ í•´ì œ

        except Exception as e:
            print(f"  - ğŸš¨ {month_str} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë‹¤ìŒ ì›”ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            continue
    
    print("\n--- ëª¨ë“  ì›” ë°ì´í„° ëˆ„ì  ì™„ë£Œ. ìµœì¢… ì €ì¥ ë° ë¶„ì„ ì‹œì‘ ---")
    try:
        # ìµœì¢… ì§‘ê³„ëœ ë°ì´í„°ë¡œ íŒŒì¼ ì €ì¥ ë° ë¶„ì„ ìˆ˜í–‰
        # 2a. ìµœì¢… ì—°ê°„ ë§í¬ë³„ í†µí–‰ëŸ‰ ì €ì¥
        if yearly_link_volume_acc:
            yearly_link_volume_acc.withColumnRenamed("count", "Yearly_Volume") \
                .withColumn("Year", F.lit(2020)).select("LINK_ID", "Yearly_Volume", "Year") \
                .coalesce(1).write.mode("overwrite").option("header", "true").csv(YEARLY_LINK_VOLUME_CSV_PATH)
            print("  - âœ… ìµœì¢… ì—°ê°„ ë§í¬ë³„ í†µí–‰ëŸ‰ CSV í´ë” ì €ì¥ ì™„ë£Œ.")
        
        # 2b. ìµœì¢… ì¼ë³„ ì´ í†µí–‰ëŸ‰ ì €ì¥
        if daily_total_volume_acc:
            daily_total_volume_acc.withColumn("Day_of_Week", F.date_format(F.col("date"), "E")) \
                .coalesce(1).write.mode("overwrite").option("header", "true").csv(DAILY_TOTAL_VOLUME_CSV_PATH)
            print("  - âœ… ìµœì¢… ì¼ë³„ ì´ í†µí–‰ëŸ‰ CSV í´ë” ì €ì¥ ì™„ë£Œ.")
        
        # 2c. ì •ì§€ ì‹œê°„ êµ°ì§‘ ë¶„ì„ ë° ì €ì¥ (ì•ˆì •ì„± ë° ì„±ëŠ¥ ê°•í™” ë²„ì „)
        print("  - 2c. ì •ì§€ ì‹œê°„ êµ°ì§‘ ë¶„ì„ ë° ì €ì¥ ì‹œì‘...")

        # ìµœì¢… ëˆ„ì ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€, ê·¸ë¦¬ê³  ë‚´ìš©ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
        if stopping_df_acc and stopping_df_acc.count() > 0:
            
            # â— [ìˆ˜ì • 1] ë¹ˆ íŒŒí‹°ì…˜ ë¬¸ì œë¥¼ ì›ì²œ ì°¨ë‹¨í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ë‹¨ì¼ íŒŒí‹°ì…˜ìœ¼ë¡œ í†µí•©
            # dropna()ë¡œ null ê°’ì„ ë¨¼ì € ì œê±°í•˜ê³ , coalesce(1)ë¡œ íŒŒí‹°ì…˜ì„ í•˜ë‚˜ë¡œ í•©ì³ ì•ˆì •ì„± í™•ë³´
            stopping_df_final = stopping_df_acc.dropna(subset=["STOPPING_TIME"]).coalesce(1)
            
            # â— [ìˆ˜ì • 2] ë°˜ë³µì ì¸ ML í•™ìŠµì„ ìœ„í•´ ì •ì œëœ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ê³ ì •(ìºì‹œ)
            stopping_df_final.persist()
            
            try:
                print(f"  - âœ… ìµœì¢… êµ°ì§‘ ë¶„ì„ ëŒ€ìƒ ë°ì´í„° {stopping_df_final.count():,} ê±´ ì¤€ë¹„ ì™„ë£Œ. ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                
                pipeline = Pipeline(stages=[VectorAssembler(inputCols=["STOPPING_TIME"], outputCol="features"), KMeans(k=4, seed=1)])
                model = pipeline.fit(stopping_df_final)
                clustered_df = model.transform(stopping_df_final)
                
                cluster_summary = clustered_df.groupBy("prediction").agg(F.mean("STOPPING_TIME").alias("avg_stop_time")).orderBy("avg_stop_time")
                ranked_clusters = cluster_summary.withColumn("rank", F.row_number().over(Window.orderBy("avg_stop_time")) - 1).collect()
                
                # --- â— [ìˆ˜ì • 2] label_mapping ë° mapping_expr ìƒì„± ë°©ì‹ ë³€ê²½ ---
                
                # 1. Python ë”•ì…”ë„ˆë¦¬ë¡œ ì˜ˆì¸¡-ë ˆì´ë¸” ë§µì„ ë¨¼ì € ìƒì„±
                label_map_dict = {
                    row['prediction']: f"{stop_type}({row['rank']})" 
                    for row, stop_type in zip(ranked_clusters, ["Short Stop", "Work Stop", "End-of-Day Stop", "Long Stop"])
                }
                
                # 2. ë”•ì…”ë„ˆë¦¬ë¥¼ [í‚¤1, ê°’1, í‚¤2, ê°’2, ...] í˜•íƒœì˜ í‰í‰í•œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                flattened_map = list(itertools.chain.from_iterable(label_map_dict.items()))
                
                # 3. create_map í•¨ìˆ˜ì— ì˜¬ë°”ë¥¸ í˜•íƒœë¡œ ì „ë‹¬
                mapping_expr = F.create_map(*[F.lit(x) for x in flattened_map])
                
                # ----------------------------------------------------------------

                final_stops_df = clustered_df.withColumn("stop_type_label", mapping_expr[F.col("prediction")])
                final_stops_df.write.mode("overwrite").parquet(CLUSTERED_STOPS_PARQUET_PATH)
                
                print("  - âœ… ì •ì§€ ì‹œê°„ êµ°ì§‘ ë¶„ì„ ë°ì´í„° Parquet ì €ì¥ ì™„ë£Œ.")

            finally:
                stopping_df_final.unpersist()
                
        else:
            print("  - âš ï¸ ìµœì¢… ëˆ„ì ëœ ì •ì§€ ë°ì´í„°ê°€ ì—†ì–´ K-Means êµ°ì§‘ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

    finally:
        spark.stop()
        print("\n--- Spark ì„¸ì…˜ ì¢…ë£Œ ---")


# --- 3. ì‹œê°í™” í•¨ìˆ˜ ---
def create_traffic_heatmap():
    """(5ë¶„ìœ„ìˆ˜ ë²„ì „) ë„ë¡œ ë„¤íŠ¸ì›Œí¬ í†µí–‰ëŸ‰ Heatmapì„ í°ìƒ‰ ë°°ê²½ê³¼ 5ë¶„ìœ„ ìƒ‰ìƒìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    print("\n--- 3a. ë„ë¡œ ë„¤íŠ¸ì›Œí¬ Heatmap ìƒì„± ì‹œì‘ (5ë¶„ìœ„ìˆ˜ ë°©ì‹) ---")
    try:
        csv_folder_path = YEARLY_LINK_VOLUME_CSV_PATH
        if not os.path.exists(csv_folder_path):
            print(f"ê²½ê³ : Heatmapì„ ìœ„í•œ CSV í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {csv_folder_path}. ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]
        if not csv_files: raise FileNotFoundError(f"CSV íŒŒì¼ì´ í•´ë‹¹ í´ë”ì— ì—†ìŠµë‹ˆë‹¤: {csv_folder_path}")
        csv_file_path = os.path.join(csv_folder_path, csv_files[0])
        
        gdf_roads = gpd.read_file(SHAPEFILE_PATH)
        df_volume = pd.read_csv(csv_file_path)
        
        gdf_roads['link_id'] = gdf_roads['link_id'].astype(str)
        df_volume.rename(columns={'LINK_ID': 'link_id'}, inplace=True)
        df_volume['link_id'] = df_volume['link_id'].astype(str)
        merged_gdf = gdf_roads.merge(df_volume, on='link_id', how='inner')
        if merged_gdf.crs != TARGET_CRS: merged_gdf = merged_gdf.to_crs(TARGET_CRS)
        
        map_data = merged_gdf[merged_gdf['Yearly_Volume'] > 0].copy()
        print(f"  - ëª¨ë“  ë„ë¡œ ë§í¬({len(map_data):,}ê°œ)ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.")

        # â— [ìˆ˜ì •] LinearColormapì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ê°ì²´ ìƒì„±
        custom_colors = ['blue', 'green', 'yellow', 'orange', 'red']
        colormap = LinearColormap(
            colors=custom_colors,
            vmin=map_data['Yearly_Volume'].min(),
            vmax=map_data['Yearly_Volume'].quantile(0.95)
        )
        colormap.caption = 'Yearly Freight Truck Volume'
        
        m = folium.Map(location=[36.5, 127.8], zoom_start=7, tiles='CartoDB positron', control_scale=True)
        
        folium.GeoJson(
            map_data,
            style_function=lambda x: {
                'color': colormap(x['properties']['Yearly_Volume']),
                'weight': 2.5,
                'opacity': 0.8
            },
            tooltip=folium.GeoJsonTooltip(
                fields=['link_id', 'road_name', 'Yearly_Volume'],
                aliases=['Link ID:', 'Road Name:', 'Yearly Volume:']
            )
        ).add_to(m)
        
        m.add_child(colormap)
        m.save(HEATMAP_HTML)
        print(f"  - âœ… Heatmap HTML ì €ì¥ ì™„ë£Œ: {HEATMAP_HTML}")

    except Exception as e:
        print(f"ì˜¤ë¥˜: Heatmap ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")

def create_daily_volume_boxplot():
    """ìš”ì¼ë³„ í†µí–‰ëŸ‰ Boxplotì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("\n--- 3b. ìš”ì¼ë³„ í†µí–‰ëŸ‰ Boxplot ìƒì„± ì‹œì‘ ---")
    try:
        csv_folder_path = DAILY_TOTAL_VOLUME_CSV_PATH
        if not os.path.exists(csv_folder_path):
            print(f"ê²½ê³ : Boxplotì„ ìœ„í•œ CSV í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {csv_folder_path}. ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]
        if not csv_files: raise FileNotFoundError(f"CSV íŒŒì¼ì´ í•´ë‹¹ í´ë”ì— ì—†ìŠµë‹ˆë‹¤: {csv_folder_path}")
        csv_file_path = os.path.join(csv_folder_path, csv_files[0])

        df = pd.read_csv(csv_file_path)
        day_order = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
        df['Day_of_Week'] = pd.Categorical(df['Day_of_Week'], categories=day_order, ordered=True)
        
        fig, ax = plt.subplots(figsize=(16, 9))
        sns.boxplot(x='Day_of_Week', y='Daily_Volume', data=df, ax=ax, palette='viridis', width=0.6)

        for i, day in enumerate(day_order):
            subset = df[df['Day_of_Week'] == day]
            if subset.empty: continue
            min_v, max_v, avg_v, n_v = subset['Daily_Volume'].min(), subset['Daily_Volume'].max(), subset['Daily_Volume'].mean(), len(subset)
            stat_text = f"Min: {min_v:,.0f}\nMax: {max_v:,.0f}\nAvg: {avg_v:,.1f}\nN = {n_v}"
            ax.text(i, max_v * 1.05, stat_text, ha='center', va='bottom', bbox=dict(boxstyle='round,pad=0.5', fc='aliceblue', alpha=0.8), fontsize=10)
        
        ax.set_title('ìš”ì¼ë³„ ì¼ì¼ í†µí–‰ëŸ‰ ë¶„í¬ (2020ë…„)', fontsize=20, pad=20)
        ax.set_xlabel('ìš”ì¼ (Day of Week)', fontsize=14)
        ax.set_ylabel('ì¼ì¼ í†µí–‰ëŸ‰ (Daily Volume)', fontsize=14)
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))
        ax.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.savefig(BOXPLOT_PNG, dpi=150)
        print(f"  - âœ… Boxplot ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {BOXPLOT_PNG}")

    except Exception as e:
        print(f"ì˜¤ë¥˜: Boxplot ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")


def create_stop_duration_scatter_plot():
    """ì •ì§€ ìœ í˜•ë³„ ì§€ì† ì‹œê°„ Scatter Plotì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("\n--- 3c. ì •ì§€ ìœ í˜•ë³„ Scatter Plot ìƒì„± ì‹œì‘ ---")
    try:
        if not os.path.exists(CLUSTERED_STOPS_PARQUET_PATH):
             print(f"ê²½ê³ : Scatter Plotì„ ìœ„í•œ Parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {CLUSTERED_STOPS_PARQUET_PATH}. ì´ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
             return

        df = pd.read_parquet(CLUSTERED_STOPS_PARQUET_PATH)
        
        sampled_df = df.sample(frac=0.00001, random_state=42).reset_index(drop=True)
        sampled_df['Stop_Duration_Hours'] = sampled_df['STOPPING_TIME'] / 60
        
        type_order = sorted(sampled_df['stop_type_label'].unique(), key=lambda x: int(x[-2]))
        
        plt.figure(figsize=(18, 9))
        sns.scatterplot(
            data=sampled_df, x=sampled_df.index, y='Stop_Duration_Hours',
            hue='stop_type_label', style='stop_type_label',
            hue_order=type_order, style_order=type_order, s=50
        )
        
        plt.title('ì •ì§€ ìœ í˜•ë³„ ì§€ì† ì‹œê°„ ë¶„í¬ (0.1% ìƒ˜í”Œë§)', fontsize=16)
        plt.xlabel('ì •ì§€ ìˆœì„œ ì¸ë±ìŠ¤ (ìƒ˜í”Œë§)', fontsize=12)
        plt.ylabel('ì •ì§€ ì§€ì† ì‹œê°„ (ì‹œê°„)', fontsize=12)
        plt.legend(title='ì •ì§€ ìœ í˜• (Stop Type)')
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.tight_layout()
        plt.savefig(SCATTER_PLOT_PNG, dpi=150)
        print(f"  - âœ… Scatter Plot ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {SCATTER_PLOT_PNG}")
        
    except Exception as e:
        print(f"ì˜¤ë¥˜: Scatter Plot ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")


# --- 4. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
if __name__ == "__main__":
    total_start_time = time.time()
    
    # Spark ë¶„ì„ ì‹¤í–‰
    run_spark_analysis()
    
    # ì‹œê°í™” ì‹¤í–‰
    create_traffic_heatmap()
    create_daily_volume_boxplot()
    create_stop_duration_scatter_plot()
    
    print(f"\nâœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ. ì´ ì†Œìš” ì‹œê°„: {(time.time() - total_start_time) / 60:.2f} ë¶„.")