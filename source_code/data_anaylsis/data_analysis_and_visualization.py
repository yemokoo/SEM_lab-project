# -*- coding: utf-8 -*-
import os
import time
import pandas as pd
from pyspark.sql import SparkSession, Window
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
from pyspark.sql import functions as F
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
import geopandas as gpd
import folium
import seaborn as sns
import matplotlib.pyplot as plt
from branca.colormap import LinearColormap
import itertools

# --- 1. Í≤ΩÎ°ú Î∞è ÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï ---
print("--- 1. Í≤ΩÎ°ú Î∞è ÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï ---")

PARQUET_BASE_PATH = r"D:\Ïó∞Íµ¨Ïã§\Ïó∞Íµ¨\ÌôîÎ¨ºÏ∞® Ï∂©Ï†ÑÏÜå Î∞∞Ïπò ÏµúÏ†ÅÌôî\Data\Processed_Data\simulator\Trajectory(MONTH_FULL)"
SHAPEFILE_PATH = r"D:\Ïó∞Íµ¨Ïã§\Ïó∞Íµ¨\ÌôîÎ¨ºÏ∞® Ï∂©Ï†ÑÏÜå Î∞∞Ïπò ÏµúÏ†ÅÌôî\Data\Raw_Data\main_road_network_level_5.5\level5_5_link_probe_32_2020.shp"
BASE_OUTPUT_DIR = r"D:\Ïó∞Íµ¨Ïã§\Ïó∞Íµ¨\ÌôîÎ¨ºÏ∞® Ï∂©Ï†ÑÏÜå Î∞∞Ïπò ÏµúÏ†ÅÌôî\Data\Processed_Data\Data_Analysis"

STATISTICS_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, "Statistics")
VISUALIZATION_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, "Visualization")

# ÌÜµÍ≥Ñ ÌååÏùº Í≤ΩÎ°ú
YEARLY_LINK_VOLUME_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "link_yearly_volume_2020")
DAILY_TOTAL_VOLUME_WITH_HOLIDAYS_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "daily_total_volume_with_holidays")
DAILY_TOTAL_VOLUME_WITHOUT_HOLIDAYS_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "daily_total_volume_without_holidays")
CLUSTERED_STOPS_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "clustered_stops_csv")
DAILY_DISTANCE_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "daily_driving_distance")
DAILY_DISTANCE_STATS_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "daily_driving_distance_stats.csv")
ALL_DRIVING_SEGMENTS_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "all_driving_segments")

# ÏãúÍ∞ÅÌôî ÌååÏùº Í≤ΩÎ°ú
HEATMAP_VIS_DIR = os.path.join(VISUALIZATION_OUTPUT_DIR, "Heatmaps")
BOXPLOT_WITH_HOLIDAYS_PNG = os.path.join(VISUALIZATION_OUTPUT_DIR, "figure2_daily_volume_boxplot_with_holidays_en.png")
BOXPLOT_WITHOUT_HOLIDAYS_PNG = os.path.join(VISUALIZATION_OUTPUT_DIR, "figure2_daily_volume_boxplot_without_holidays_en.png")
SCATTER_PLOT_PNG = os.path.join(VISUALIZATION_OUTPUT_DIR, "figure3_stop_duration_scatter_en.png")
DAILY_DISTANCE_HISTOGRAM_PNG = os.path.join(VISUALIZATION_OUTPUT_DIR, "figure4_daily_distance_histogram.png")
STOP_LOCATION_MAP_DIR = os.path.join(VISUALIZATION_OUTPUT_DIR, "Stop_Location_Maps")

TARGET_CRS = "EPSG:4326"

os.makedirs(STATISTICS_OUTPUT_DIR, exist_ok=True)
os.makedirs(VISUALIZATION_OUTPUT_DIR, exist_ok=True)
os.makedirs(HEATMAP_VIS_DIR, exist_ok=True)
os.makedirs(STOP_LOCATION_MAP_DIR, exist_ok=True)

processed_schema = StructType([
    StructField("OBU_ID", IntegerType(), True), StructField("DATETIME", StringType(), True),
    StructField("LINK_ID", IntegerType(), True), StructField("LINK_LENGTH", DoubleType(), True),
    StructField("DRIVING_TIME_MINUTES", DoubleType(), True), StructField("STOPPING_TIME", DoubleType(), True),
    StructField("CUMULATIVE_DRIVING_TIME_MINUTES", DoubleType(), True), StructField("CUMULATIVE_STOPPING_TIME_MINUTES", DoubleType(), True),
    StructField("CUMULATIVE_LINK_LENGTH", DoubleType(), True), StructField("SIGUNGU_ID", StringType(), True),
    StructField("AREA_ID", IntegerType(), True), StructField("START_TIME_MINUTES", DoubleType(), True),
])


# --- 2. SparkÏùÑ Ïù¥Ïö©Ìïú Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù ---
def run_spark_analysis():
    """SparkÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ïó∞Í∞Ñ/ÏùºÎ≥Ñ ÌÜµÌñâÎüâ, Ï†ïÏßÄ Ïú†Ìòï, ÏùºÏùº Ï£ºÌñâÍ±∞Î¶¨ Î∂ÑÏÑùÏùÑ ÏàòÌñâÌï©ÎãàÎã§."""
    print("\n--- 2. Spark Î∂ÑÏÑù ÏãúÏûë (ÏõîÎ≥Ñ ÏàúÏ∞® Ï≤òÎ¶¨) ---")
    
    spark = SparkSession.builder.appName("CombinedTrafficAnalysis_Monthly").config("spark.driver.memory", "32g").config("spark.driver.host", "127.0.0.1").getOrCreate()
    
    yearly_link_volume_acc, daily_total_volume_acc, stopping_df_acc, daily_distance_acc = None, None, None, None
    all_driving_segments_acc = None
    
    for month in range(1, 13):
        year, month_str = 2020, f"2020-{month:02d}"
        monthly_path = os.path.join(PARQUET_BASE_PATH, month_str)
        print(f"\n--- {month_str} Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÏãúÏûë ---")
        
        try:
            df_month = spark.read.schema(processed_schema).parquet(monthly_path)
            df_month.cache()
            count = df_month.count()
            if count == 0:
                print(f"  - {month_str} Ìè¥ÎçîÏóê Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§. Í±¥ÎÑàÎúÅÎãàÎã§."); df_month.unpersist(); continue
            print(f"  - {month_str} Îç∞Ïù¥ÌÑ∞ {count:,} Í±¥ Î°úÎìú ÏôÑÎ£å.")

            monthly_link_volume = df_month.groupBy("LINK_ID").count()
            if yearly_link_volume_acc is None: yearly_link_volume_acc = monthly_link_volume
            else: yearly_link_volume_acc = yearly_link_volume_acc.join(monthly_link_volume, "LINK_ID", "full_outer").select(F.col("LINK_ID"), (F.coalesce(yearly_link_volume_acc["count"], F.lit(0)) + F.coalesce(monthly_link_volume["count"], F.lit(0))).alias("count"))

            monthly_daily_volume = df_month.withColumn("date", F.to_date("DATETIME")).groupBy("date").count().withColumnRenamed("count", "Daily_Volume")
            if daily_total_volume_acc is None: daily_total_volume_acc = monthly_daily_volume
            else: daily_total_volume_acc = daily_total_volume_acc.unionByName(monthly_daily_volume)

            monthly_stopping_df = df_month.filter(F.col("STOPPING_TIME") > 0)
            if stopping_df_acc is None: stopping_df_acc = monthly_stopping_df
            else: stopping_df_acc = stopping_df_acc.unionByName(monthly_stopping_df)
            
            monthly_daily_distance = df_month.withColumn("date", F.to_date("DATETIME")).groupBy("OBU_ID", "date").agg(F.sum("LINK_LENGTH").alias("daily_distance_km"))
            if daily_distance_acc is None: daily_distance_acc = monthly_daily_distance
            else: daily_distance_acc = daily_distance_acc.unionByName(monthly_daily_distance)

            if all_driving_segments_acc is None: all_driving_segments_acc = df_month
            else: all_driving_segments_acc = all_driving_segments_acc.unionByName(df_month)
            
            df_month.unpersist()
        except Exception as e:
            print(f"  - üö® {month_str} Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}. Îã§Ïùå ÏõîÎ°ú ÏßÑÌñâÌï©ÎãàÎã§."); continue
    
    print("\n--- Î™®Îì† Ïõî Îç∞Ïù¥ÌÑ∞ ÎàÑÏ†Å ÏôÑÎ£å. ÏµúÏ¢Ö Ï†ÄÏû• Î∞è Î∂ÑÏÑù ÏãúÏûë ---")
    try:
        if all_driving_segments_acc:
             all_driving_segments_acc.coalesce(1).write.mode("overwrite").option("header", "true").csv(ALL_DRIVING_SEGMENTS_CSV_PATH)
             print("  - ‚úÖ Ï†ÑÏ≤¥ Ï£ºÌñâ Í∏∞Î°ù Îç∞Ïù¥ÌÑ∞ CSV Ï†ÄÏû• ÏôÑÎ£å.")

        if yearly_link_volume_acc:
            yearly_link_volume_acc.withColumnRenamed("count", "Yearly_Volume").withColumn("Year", F.lit(2020)).coalesce(1).write.mode("overwrite").option("header", "true").csv(YEARLY_LINK_VOLUME_CSV_PATH)
            print("  - ‚úÖ ÏµúÏ¢Ö Ïó∞Í∞Ñ ÎßÅÌÅ¨Î≥Ñ ÌÜµÌñâÎüâ CSV Ï†ÄÏû• ÏôÑÎ£å.")
        
        if daily_total_volume_acc:
            daily_total_volume_acc.withColumn("Day_of_Week", F.date_format(F.col("date"), "E")).coalesce(1).write.mode("overwrite").option("header", "true").csv(DAILY_TOTAL_VOLUME_WITH_HOLIDAYS_CSV_PATH)
            holidays_2020 = ['2020-01-01', '2020-01-24', '2020-01-25', '2020-01-26', '2020-01-27', '2020-03-01', '2020-04-15', '2020-04-30', '2020-05-01', '2020-05-05', '2020-06-06', '2020-08-15', '2020-08-17', '2020-09-30', '2020-10-01', '2020-10-02', '2020-10-03', '2020-10-09', '2020-12-25']
            daily_total_volume_acc.filter(~F.col("date").cast("string").isin(holidays_2020)).withColumn("Day_of_Week", F.date_format(F.col("date"), "E")).coalesce(1).write.mode("overwrite").option("header", "true").csv(DAILY_TOTAL_VOLUME_WITHOUT_HOLIDAYS_CSV_PATH)
            print("  - ‚úÖ ÏµúÏ¢Ö ÏùºÎ≥Ñ ÌÜµÌñâÎüâ(Í≥µÌú¥Ïùº Ìè¨Ìï®/Ï†úÏô∏) CSV 2Ï¢Ö Ï†ÄÏû• ÏôÑÎ£å.")
            
        if daily_distance_acc:
            daily_distance_acc.coalesce(1).write.mode("overwrite").option("header", "true").csv(DAILY_DISTANCE_CSV_PATH)
            print("  - ‚úÖ ÏµúÏ¢Ö OBUÎ≥Ñ ÏùºÏùº Ï£ºÌñâÍ±∞Î¶¨ CSV Ï†ÄÏû• ÏôÑÎ£å.")

        if stopping_df_acc and stopping_df_acc.count() > 0:
            stopping_df_final = stopping_df_acc.dropna(subset=["STOPPING_TIME"]).coalesce(1)
            stopping_df_final.persist()
            try:
                print(f"  - ‚úÖ ÏµúÏ¢Ö Íµ∞Ïßë Î∂ÑÏÑù ÎåÄÏÉÅ Îç∞Ïù¥ÌÑ∞ {stopping_df_final.count():,} Í±¥ Ï§ÄÎπÑ ÏôÑÎ£å.")
                pipeline = Pipeline(stages=[VectorAssembler(inputCols=["STOPPING_TIME"], outputCol="features"), KMeans(k=4, seed=1)])
                model = pipeline.fit(stopping_df_final)
                clustered_df = model.transform(stopping_df_final)
                cluster_summary = clustered_df.groupBy("prediction").agg(F.mean("STOPPING_TIME").alias("avg_stop_time")).orderBy("avg_stop_time")
                ranked_clusters = cluster_summary.withColumn("rank", F.row_number().over(Window.orderBy("avg_stop_time")) - 1).collect()
                label_map_dict = {row['prediction']: f"{stop_type}({row['rank']})" for row, stop_type in zip(ranked_clusters, ["Short Stop", "Work Stop", "End-of-Day Stop", "Long Stop"])}
                flattened_map = list(itertools.chain.from_iterable(label_map_dict.items()))
                mapping_expr = F.create_map(*[F.lit(x) for x in flattened_map])
                final_stops_df = clustered_df.withColumn("stop_type_label", mapping_expr[F.col("prediction")])
                
                final_stops_df.drop("features", "prediction") \
                    .coalesce(1).write.mode("overwrite").option("header", "true").csv(CLUSTERED_STOPS_CSV_PATH)
                    
                print("  - ‚úÖ Ï†ïÏßÄ ÏãúÍ∞Ñ Íµ∞Ïßë Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ CSV Ï†ÄÏû• ÏôÑÎ£å.")
            finally: 
                stopping_df_final.unpersist()
        else: 
            print("  - ‚ö†Ô∏è ÏµúÏ¢Ö ÎàÑÏ†ÅÎêú Ï†ïÏßÄ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏñ¥ K-Means Íµ∞Ïßë Î∂ÑÏÑùÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.")
    finally:
        spark.stop()
        print("\n--- Spark ÏÑ∏ÏÖò Ï¢ÖÎ£å ---")


# --- 3. ÏãúÍ∞ÅÌôî Ìï®Ïàò ---
def create_traffic_heatmap():
    """Ïó¨Îü¨ Ï°∞Í±¥Ïóê Îî∞Îùº Îã§ÏñëÌïú Î≤ÑÏ†ÑÏùò HeatmapÏùÑ ÏÉùÏÑ±Ìï©ÎãàÎã§."""
    print("\n--- 3a. ÎèÑÎ°ú ÎÑ§Ìä∏ÏõåÌÅ¨ Heatmap ÏÉùÏÑ± ÏãúÏûë ---")
    try:
        csv_folder_path = YEARLY_LINK_VOLUME_CSV_PATH
        if not os.path.exists(csv_folder_path):
            print(f"Í≤ΩÍ≥†: Heatmap CSV Ìè¥ÎçîÍ∞Ä ÏóÜÏäµÎãàÎã§: {csv_folder_path}. Í±¥ÎÑàÎúÅÎãàÎã§."); return
        csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]
        if not csv_files: raise FileNotFoundError(f"CSV ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§: {csv_folder_path}")
        csv_file_path = os.path.join(csv_folder_path, csv_files[0])
        gdf_roads, df_volume = gpd.read_file(SHAPEFILE_PATH), pd.read_csv(csv_file_path)
        
        gdf_roads['link_id'] = gdf_roads['link_id'].astype(str)
        df_volume.rename(columns={'LINK_ID': 'link_id'}, inplace=True)
        df_volume['link_id'] = df_volume['link_id'].astype(str)
        merged_gdf = gdf_roads.merge(df_volume, on='link_id', how='inner')
        if merged_gdf.crs != TARGET_CRS: merged_gdf = merged_gdf.to_crs(TARGET_CRS)
        
        map_data = merged_gdf[merged_gdf['Yearly_Volume'] > 0].copy()
        map_data['quantile_all'] = pd.qcut(map_data['Yearly_Volume'], q=5, labels=False, duplicates='drop')
        color_dict = {0: 'blue', 1: 'green', 2: 'yellow', 3: 'orange', 4: 'red'}
        print("  - ‚úÖ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ Î∞è Ï†ÑÏ≤¥ 5Î∂ÑÏúÑÏàò Í≥ÑÏÇ∞ ÏôÑÎ£å.")

        print("\n  --- Îì±Í∏âÎ≥Ñ ÎàÑÏ†Å ÏßÄÎèÑ ÏÉùÏÑ± Ï§ë ---")
        style_function_all = lambda x: {'color': color_dict.get(x['properties']['quantile_all'], 'grey'), 'weight': 2.5, 'opacity': 0.8}
        for i in range(4, -1, -1):
            filtered_data = map_data[map_data['quantile_all'] >= i]
            percentage = (5 - i) * 20
            filename = os.path.join(HEATMAP_VIS_DIR, f"heatmap_top{percentage}pct.html")
            if i == 0: filename = os.path.join(HEATMAP_VIS_DIR, "heatmap_all.html")
            print(f"    - ÏÉÅÏúÑ {percentage}% ÏßÄÎèÑ ÏÉùÏÑ± Ï§ë... ({len(filtered_data):,}Í∞ú ÎßÅÌÅ¨)")
            m = folium.Map(location=[36.5, 127.8], zoom_start=7, tiles='CartoDB positron', control_scale=True)
            folium.GeoJson(filtered_data, style_function=style_function_all, tooltip=folium.GeoJsonTooltip(fields=['link_id', 'road_name', 'Yearly_Volume', 'quantile_all'], aliases=['Link ID:', 'Road Name:', 'Yearly Volume:', 'Overall Quantile:'])).add_to(m)
            m.save(filename)

        print("\n  --- ÌÜµÌñâÎüâ ÏÉÅÏúÑ 25% ÏßÄÎèÑ ÏÉùÏÑ± Ï§ë ---")
        top_25_threshold = map_data['Yearly_Volume'].quantile(0.75)
        top_25_data = map_data[map_data['Yearly_Volume'] >= top_25_threshold].copy()
        top_25_data['quantile_top25'] = pd.qcut(top_25_data['Yearly_Volume'], q=5, labels=False, duplicates='drop')
        style_function_top25 = lambda x: {'color': color_dict.get(x['properties']['quantile_top25'], 'grey'),'weight': 2.5, 'opacity': 0.8}
        filename = os.path.join(HEATMAP_VIS_DIR, "heatmap_top25vol.html")
        print(f"    - ÏÉÅÏúÑ 25% ÏßÄÎèÑ ÏÉùÏÑ± Ï§ë... ({len(top_25_data):,}Í∞ú ÎßÅÌÅ¨)")
        m_top25 = folium.Map(location=[36.5, 127.8], zoom_start=7, tiles='CartoDB positron', control_scale=True)
        folium.GeoJson(top_25_data, style_function=style_function_top25, tooltip=folium.GeoJsonTooltip(fields=['link_id', 'road_name', 'Yearly_Volume', 'quantile_top25'], aliases=['Link ID:', 'Road Name:', 'Yearly Volume:', 'Top 25% Quantile:'])).add_to(m_top25)
        m_top25.save(filename)
        print(f"  - ‚úÖ Î™®Îì† Heatmap Ï†ÄÏû• ÏôÑÎ£å.")

    except Exception as e:
        print(f"Ïò§Î•ò: Heatmap ÏÉùÏÑ± Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§ - {e}")

def create_daily_volume_boxplot():
    """Í≥µÌú¥Ïùº Ìè¨Ìï®/Ï†úÏô∏ Îëê Í∞ÄÏßÄ Î≤ÑÏ†ÑÏùò BoxplotÏùÑ Î™®Îëê ÏÉùÏÑ±Ìï©ÎãàÎã§."""
    print("\n--- 3b. ÏöîÏùºÎ≥Ñ ÌÜµÌñâÎüâ Boxplot ÏÉùÏÑ± ÏãúÏûë ---")
    plot_configs = [
        {"title": "Daily Volume Distribution by Day of Week (Holidays INCLUDED)", "csv_path": DAILY_TOTAL_VOLUME_WITH_HOLIDAYS_CSV_PATH, "output_png": BOXPLOT_WITH_HOLIDAYS_PNG},
        {"title": "Daily Volume Distribution by Day of Week (Holidays EXCLUDED)", "csv_path": DAILY_TOTAL_VOLUME_WITHOUT_HOLIDAYS_CSV_PATH, "output_png": BOXPLOT_WITHOUT_HOLIDAYS_PNG}
    ]
    for config in plot_configs:
        try:
            print(f"\n  - '{config['title']}' Í∑∏ÎûòÌîÑ ÏÉùÏÑ± Ï§ë...")
            csv_folder_path = config["csv_path"]
            if not os.path.exists(csv_folder_path):
                print(f"    Í≤ΩÍ≥†: CSV Ìè¥ÎçîÍ∞Ä ÏóÜÏäµÎãàÎã§: {csv_folder_path}. Í±¥ÎÑàÎúÅÎãàÎã§."); continue
            csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]
            if not csv_files: raise FileNotFoundError(f"CSV ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§: {csv_folder_path}")
            csv_file_path = os.path.join(csv_folder_path, csv_files[0])
            df = pd.read_csv(csv_file_path)
            day_order = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
            df['Day_of_Week'] = pd.Categorical(df['Day_of_Week'], categories=day_order, ordered=True)
            
            fig, ax = plt.subplots(figsize=(16, 10))
            sns.boxplot(x='Day_of_Week', y='Daily_Volume', data=df, ax=ax, palette='viridis', width=0.6, hue='Day_of_Week', legend=False)
            max_y_for_text = 0
            for i, day in enumerate(day_order):
                subset = df[df['Day_of_Week'] == day]
                if subset.empty: continue
                min_v, max_v, avg_v, n_v = subset['Daily_Volume'].min(), subset['Daily_Volume'].max(), subset['Daily_Volume'].mean(), len(subset)
                stat_text = f"Min: {min_v:,.0f}\nMax: {max_v:,.0f}\nAvg: {avg_v:,.1f}\nN = {n_v}"
                text_y_position = max_v
                max_y_for_text = max(max_y_for_text, text_y_position)
                ax.text(i, text_y_position * 1.05, stat_text, ha='center', va='bottom', bbox=dict(boxstyle='round,pad=0.5', fc='aliceblue', alpha=0.8), fontsize=10)
            
            if max_y_for_text > 0: ax.set_ylim(top=max_y_for_text * 1.35)
            
            ax.set_title(config["title"], fontsize=20, pad=20)
            ax.set_xlabel('Day of Week', fontsize=14); ax.set_ylabel('Daily Volume', fontsize=14)
            ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))
            ax.grid(axis='y', linestyle='--', alpha=0.7)
            
            plt.tight_layout(); plt.savefig(config["output_png"], dpi=150); plt.close(fig)
            print(f"    ‚úÖ Boxplot Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• ÏôÑÎ£å: {config['output_png']}")
        except Exception as e:
            print(f"    Ïò§Î•ò: '{config['title']}' Í∑∏ÎûòÌîÑ ÏÉùÏÑ± Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§ - {e}")

def create_daily_distance_histogram():
    """ÏùºÏùº Ï£ºÌñâÍ±∞Î¶¨ Î∂ÑÌè¨ ÌûàÏä§ÌÜ†Í∑∏Îû®ÏùÑ ÏÉùÏÑ±ÌïòÍ≥†, ÏöîÏïΩ ÌÜµÍ≥ÑÎüâÏùÑ CSVÎ°ú Ï†ÄÏû•Ìï©ÎãàÎã§."""
    print("\n--- 3c. ÏùºÏùº Ï£ºÌñâÍ±∞Î¶¨ ÌûàÏä§ÌÜ†Í∑∏Îû® Î∞è ÌÜµÍ≥Ñ Î∂ÑÏÑù ÏãúÏûë ---")
    try:
        csv_folder_path = DAILY_DISTANCE_CSV_PATH
        if not os.path.exists(csv_folder_path):
            print(f"Í≤ΩÍ≥†: ÏùºÏùº Ï£ºÌñâÍ±∞Î¶¨ CSV Ìè¥ÎçîÍ∞Ä ÏóÜÏäµÎãàÎã§: {csv_folder_path}. Í±¥ÎÑàÎúÅÎãàÎã§."); return
        csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]
        if not csv_files: raise FileNotFoundError(f"CSV ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§: {csv_folder_path}")
        csv_file_path = os.path.join(csv_folder_path, csv_files[0])
        df = pd.read_csv(csv_file_path)

        daily_distance_stats = df['daily_distance_km'].describe()
        daily_distance_stats.to_csv(DAILY_DISTANCE_STATS_CSV_PATH)
        print(f"  - ‚úÖ ÏùºÏùº Ï£ºÌñâÍ±∞Î¶¨ ÏöîÏïΩ ÌÜµÍ≥Ñ Ï†ÄÏû• ÏôÑÎ£å: {os.path.basename(DAILY_DISTANCE_STATS_CSV_PATH)}")
        
        plt.figure(figsize=(12, 7))
        upper_bound = df['daily_distance_km'].quantile(0.99)
        sns.histplot(df[df['daily_distance_km'] <= upper_bound]['daily_distance_km'], bins=50, kde=True)
        avg_dist = df['daily_distance_km'].mean()
        plt.axvline(avg_dist, color='red', linestyle='--', label=f'Average: {avg_dist:.1f} km')
        plt.title('Distribution of Daily Driving Distance per Truck (up to 99th percentile)', fontsize=16)
        plt.xlabel('Daily Driving Distance (km)', fontsize=12); plt.ylabel('Number of Daily Trips', fontsize=12)
        plt.legend(); plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout(); plt.savefig(DAILY_DISTANCE_HISTOGRAM_PNG, dpi=150); plt.close()
        print(f"  - ‚úÖ ÏùºÏùº Ï£ºÌñâÍ±∞Î¶¨ ÌûàÏä§ÌÜ†Í∑∏Îû® Ï†ÄÏû• ÏôÑÎ£å.")
    except Exception as e:
        print(f"Ïò§Î•ò: ÏùºÏùº Ï£ºÌñâÍ±∞Î¶¨ ÌûàÏä§ÌÜ†Í∑∏Îû® ÏÉùÏÑ± Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§ - {e}")

def create_stop_duration_scatter_plot():
    """Ï†ïÏßÄ Ïú†ÌòïÎ≥Ñ Scatter PlotÏùò Î≤îÎ°ÄÏóê ÏãúÍ∞Ñ Î≤îÏúÑÏôÄ Í∞úÏàòÎ•º Ï∂îÍ∞ÄÌïòÏó¨ 2Ï¢Ö ÏÉùÏÑ±Ìï©ÎãàÎã§."""
    print("\n--- 3d. Ï†ïÏßÄ Ïú†ÌòïÎ≥Ñ Scatter Plot ÏÉùÏÑ± ÏãúÏûë ---")
    try:
        csv_folder_path = CLUSTERED_STOPS_CSV_PATH
        if not os.path.exists(csv_folder_path):
            print(f"Í≤ΩÍ≥†: Scatter PlotÏö© CSV Ìè¥ÎçîÍ∞Ä ÏóÜÏäµÎãàÎã§. Í±¥ÎÑàÎúÅÎãàÎã§."); return
        csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]
        if not csv_files: raise FileNotFoundError(f"CSV ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§: {csv_folder_path}")
        csv_file_path = os.path.join(csv_folder_path, csv_files[0])
        df = pd.read_csv(csv_file_path)
        
        if df.empty: print(f"Í≤ΩÍ≥†: Ï†ïÏßÄ Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏñ¥ÏûàÏñ¥ Scatter PlotÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§."); return
            
        total_counts = df['stop_type_label'].value_counts()
        time_ranges = df.groupby('stop_type_label')['STOPPING_TIME'].agg(['min', 'max'])
        
        sampling_frac = 0.001
        sampled_df = df.sample(frac=sampling_frac, random_state=42).reset_index(drop=True)
        if sampled_df.empty: print(f"Í≤ΩÍ≥†: ÏÉòÌîåÎßÅÎêú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏñ¥ Scatter PlotÏùÑ ÏÉùÏÑ±Ìï† Ïàò ÏóÜÏäµÎãàÎã§."); return
        
        sampled_counts = sampled_df['stop_type_label'].value_counts()
        sampled_df['Stop_Duration_Hours'] = sampled_df['STOPPING_TIME'] / 60
        type_order_original = sorted(sampled_df['stop_type_label'].unique(), key=lambda x: int(x[-2]))

        def create_legend_label_sampled(label):
            count = sampled_counts.get(label, 0)
            time_range = time_ranges.loc[label]
            return f"{label} (n={count}) (Range: {time_range['min']:.1f}-{time_range['max']:.1f} min)"

        sampled_df['legend_label_sampled'] = sampled_df['stop_type_label'].apply(create_legend_label_sampled)
        hue_order_sampled = [create_legend_label_sampled(x) for x in type_order_original]

        plt.figure(figsize=(18, 9))
        sns.scatterplot(data=sampled_df, x=sampled_df.index, y='Stop_Duration_Hours', hue='legend_label_sampled', style='legend_label_sampled', hue_order=hue_order_sampled, style_order=hue_order_sampled, s=50)
        plt.title(f'Stop Duration vs. Sequence Index by Type (Sampled {sampling_frac:.1%}) - Sampled Counts')
        plt.xlabel('Stop Sequence Index (Sampled)'); plt.ylabel('Stop Duration (Hours)')
        plt.legend(title='Stop Type (n = count in sample)'); plt.grid(True, linestyle='--', alpha=0.6)
        plt.tight_layout()
        filename_sampled = SCATTER_PLOT_PNG.replace('.png', '_sampled_counts.png')
        plt.savefig(filename_sampled, dpi=150); plt.close()
        print(f"  - ‚úÖ Scatter Plot (ÏÉòÌîåÎßÅ Í∞úÏàò Î≤ÑÏ†Ñ) Ï†ÄÏû• ÏôÑÎ£å.")

        def create_legend_label_total(label):
            count = total_counts.get(label, 0)
            time_range = time_ranges.loc[label]
            return f"{label} (N={count}) (Range: {time_range['min']:.1f}-{time_range['max']:.1f} min)"

        sampled_df['legend_label_total'] = sampled_df['stop_type_label'].apply(create_legend_label_total)
        hue_order_total = [create_legend_label_total(x) for x in type_order_original]

        plt.figure(figsize=(18, 9))
        sns.scatterplot(data=sampled_df, x=sampled_df.index, y='Stop_Duration_Hours', hue='legend_label_total', style='legend_label_total', hue_order=hue_order_total, style_order=hue_order_total, s=50)
        plt.title(f'Stop Duration vs. Sequence Index by Type (Sampled {sampling_frac:.1%}) - Total Counts')
        plt.xlabel('Stop Sequence Index (Sampled)'); plt.ylabel('Stop Duration (Hours)')
        plt.legend(title='Stop Type (N = count in total data)'); plt.grid(True, linestyle='--', alpha=0.6)
        plt.tight_layout()
        filename_total = SCATTER_PLOT_PNG.replace('.png', '_total_counts.png')
        plt.savefig(filename_total, dpi=150); plt.close()
        print(f"  - ‚úÖ Scatter Plot (Ï†ÑÏ≤¥ Í∞úÏàò Î≤ÑÏ†Ñ) Ï†ÄÏû• ÏôÑÎ£å.")
    except Exception as e:
        print(f"Ïò§Î•ò: Scatter Plot ÏÉùÏÑ± Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§ - {e}")
        
def create_stop_location_maps():
    """Ï†ïÏßÄ Ïú†ÌòïÎ≥Ñ ÏúÑÏπò ÏßÄÎèÑÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§."""
    print("\n--- 3e. Ï†ïÏßÄ Ïú†ÌòïÎ≥Ñ ÏúÑÏπò ÏßÄÎèÑ ÏÉùÏÑ± ÏãúÏûë ---")
    try:
        csv_folder_path = CLUSTERED_STOPS_CSV_PATH
        if not os.path.exists(csv_folder_path):
            print(f"Í≤ΩÍ≥†: Ï†ïÏßÄ ÏúÑÏπò ÏßÄÎèÑÎ•º ÏúÑÌïú CSV ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§. Í±¥ÎÑàÎúÅÎãàÎã§."); return
        csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]
        if not csv_files: raise FileNotFoundError(f"CSV ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§: {csv_folder_path}")
        csv_file_path = os.path.join(csv_folder_path, csv_files[0])
        df_stops = pd.read_csv(csv_file_path)

        gdf_roads = gpd.read_file(SHAPEFILE_PATH, columns=['link_id', 'geometry'])
        gdf_roads.rename(columns={'link_id': 'LINK_ID'}, inplace=True)

        df_stops['LINK_ID'] = df_stops['LINK_ID'].astype(str)
        gdf_roads['LINK_ID'] = gdf_roads['LINK_ID'].astype(str)
        
        stop_counts = df_stops.groupby(['LINK_ID', 'stop_type_label']).size().reset_index(name='stop_count')
        merged_gdf = gdf_roads.merge(stop_counts, on='LINK_ID', how='inner')
        if merged_gdf.crs != TARGET_CRS: merged_gdf = merged_gdf.to_crs(TARGET_CRS)
        
        merged_gdf['lon'] = merged_gdf.geometry.centroid.x
        merged_gdf['lat'] = merged_gdf.geometry.centroid.y
        
        stop_types = sorted(merged_gdf['stop_type_label'].unique(), key=lambda x: int(x[-2]))
        type_colors = {'Short Stop(0)': 'blue', 'Work Stop(1)': 'green', 'End-of-Day Stop(2)': 'purple', 'Long Stop(3)': 'red'}

        for stop_type in stop_types:
            type_data = merged_gdf[merged_gdf['stop_type_label'] == stop_type]
            if type_data.empty: continue
            
            m = folium.Map(location=[36.5, 127.8], zoom_start=7, tiles='CartoDB positron')
            for _, row in type_data.iterrows():
                folium.CircleMarker(
                    location=[row['lat'], row['lon']], radius=max(2, min(10, row['stop_count'] / 5)),
                    color=type_colors.get(stop_type, 'gray'), fill=True, fill_color=type_colors.get(stop_type, 'gray'),
                    tooltip=f"Link: {row['LINK_ID']}<br>Type: {row['stop_type_label']}<br>Count: {row['stop_count']}"
                ).add_to(m)
            
            filename = os.path.join(STOP_LOCATION_MAP_DIR, f"stop_locations_{stop_type.replace('(', '').replace(')', '')}.html")
            m.save(filename)
            print(f"  - ‚úÖ {stop_type} Ï†ïÏßÄ ÏúÑÏπò ÏßÄÎèÑ Ï†ÄÏû• ÏôÑÎ£å.")
            
        m_all = folium.Map(location=[36.5, 127.8], zoom_start=7, tiles='CartoDB positron')
        for stop_type in stop_types:
            type_data = merged_gdf[merged_gdf['stop_type_label'] == stop_type]
            fg = folium.FeatureGroup(name=stop_type)
            for _, row in type_data.iterrows():
                folium.CircleMarker(
                    location=[row['lat'], row['lon']], radius=max(2, min(10, row['stop_count'] / 5)),
                    color=type_colors.get(stop_type, 'gray'), fill=True, fill_color=type_colors.get(stop_type, 'gray'),
                    tooltip=f"Link: {row['LINK_ID']}<br>Type: {row['stop_type_label']}<br>Count: {row['stop_count']}"
                ).add_to(fg)
            fg.add_to(m_all)
        
        folium.LayerControl().add_to(m_all)
        filename_all = os.path.join(STOP_LOCATION_MAP_DIR, "stop_locations_all_types.html")
        m_all.save(filename_all)
        print("  - ‚úÖ Î™®Îì† Ïú†Ìòï Ï¢ÖÌï© ÏßÄÎèÑ Ï†ÄÏû• ÏôÑÎ£å.")

    except Exception as e:
        print(f"Ïò§Î•ò: Ï†ïÏßÄ ÏúÑÏπò ÏßÄÎèÑ ÏÉùÏÑ± Ï§ë Î¨∏Ï†úÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§ - {e}")

# --- 4. Î©îÏù∏ Ïã§Ìñâ Î∏îÎ°ù ---
if __name__ == "__main__":
    total_start_time = time.time()
    
    print("\n‚ú® Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù Î∞è ÏãúÍ∞ÅÌôî Ïä§ÌÅ¨Î¶ΩÌä∏ ÏãúÏûë")
    run_spark_analysis()
    
    create_traffic_heatmap()
    create_daily_volume_boxplot()
    create_daily_distance_histogram()
    create_stop_duration_scatter_plot()
    create_stop_location_maps()
    
    print(f"\n‚ú® Î™®Îì† ÏûëÏóÖ ÏôÑÎ£å. Ï¥ù ÏÜåÏöî ÏãúÍ∞Ñ: {(time.time() - total_start_time) / 60:.2f} Î∂Ñ.")