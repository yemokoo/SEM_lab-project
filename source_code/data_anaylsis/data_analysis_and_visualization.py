# -*- coding: utf-8 -*-
import os
import time
import pandas as pd
from pyspark.sql import SparkSession, Window
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
from pyspark.sql import functions as F
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
import geopandas as gpd
import folium
import seaborn as sns
import matplotlib.pyplot as plt
from branca.colormap import LinearColormap
import itertools

# --- 1. ê²½ë¡œ ë° íŒŒë¼ë¯¸í„° ì„¤ì • ---
print("--- 1. ê²½ë¡œ ë° íŒŒë¼ë¯¸í„° ì„¤ì • ---")

PARQUET_BASE_PATH = r"D:\ì—°êµ¬ì‹¤\ì—°êµ¬\í™”ë¬¼ì°¨ ì¶©ì „ì†Œ ë°°ì¹˜ ìµœì í™”\Data\Processed_Data\simulator\Trajectory(MONTH_FULL)"
SHAPEFILE_PATH = r"D:\ì—°êµ¬ì‹¤\ì—°êµ¬\í™”ë¬¼ì°¨ ì¶©ì „ì†Œ ë°°ì¹˜ ìµœì í™”\Data\Raw_Data\main_road_network_level_5.5\level5_5_link_probe_32_2020.shp"
BASE_OUTPUT_DIR = r"D:\ì—°êµ¬ì‹¤\ì—°êµ¬\í™”ë¬¼ì°¨ ì¶©ì „ì†Œ ë°°ì¹˜ ìµœì í™”\Data\Processed_Data\Data_Analysis"

STATISTICS_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, "Statistics")
VISUALIZATION_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, "Visualization")

# í†µê³„ íŒŒì¼ ê²½ë¡œ
YEARLY_LINK_VOLUME_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "link_yearly_volume_2020")
DAILY_TOTAL_VOLUME_WITH_HOLIDAYS_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "daily_total_volume_with_holidays")
DAILY_TOTAL_VOLUME_WITHOUT_HOLIDAYS_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "daily_total_volume_without_holidays")
CLUSTERED_STOPS_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "clustered_stops_csv")
DAILY_DISTANCE_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "daily_driving_distance")
DAILY_DISTANCE_STATS_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "daily_driving_distance_stats.csv")
ALL_DRIVING_SEGMENTS_CSV_PATH = os.path.join(STATISTICS_OUTPUT_DIR, "all_driving_segments")

# ì‹œê°í™” íŒŒì¼ ê²½ë¡œ
HEATMAP_VIS_DIR = os.path.join(VISUALIZATION_OUTPUT_DIR, "Heatmaps")
BOXPLOT_WITH_HOLIDAYS_PNG = os.path.join(VISUALIZATION_OUTPUT_DIR, "figure2_daily_volume_boxplot_with_holidays_en.png")
BOXPLOT_WITHOUT_HOLIDAYS_PNG = os.path.join(VISUALIZATION_OUTPUT_DIR, "figure2_daily_volume_boxplot_without_holidays_en.png")
SCATTER_PLOT_PNG = os.path.join(VISUALIZATION_OUTPUT_DIR, "figure3_stop_duration_scatter_en.png")
DAILY_DISTANCE_HISTOGRAM_PNG = os.path.join(VISUALIZATION_OUTPUT_DIR, "figure4_daily_distance_histogram.png")
STOP_LOCATION_MAP_DIR = os.path.join(VISUALIZATION_OUTPUT_DIR, "Stop_Location_Maps")

TARGET_CRS = "EPSG:4326"

os.makedirs(STATISTICS_OUTPUT_DIR, exist_ok=True)
os.makedirs(VISUALIZATION_OUTPUT_DIR, exist_ok=True)
os.makedirs(HEATMAP_VIS_DIR, exist_ok=True)
os.makedirs(STOP_LOCATION_MAP_DIR, exist_ok=True)

processed_schema = StructType([
    StructField("OBU_ID", IntegerType(), True), StructField("DATETIME", StringType(), True),
    StructField("LINK_ID", IntegerType(), True), StructField("LINK_LENGTH", DoubleType(), True),
    StructField("DRIVING_TIME_MINUTES", DoubleType(), True), StructField("STOPPING_TIME", DoubleType(), True),
    StructField("CUMULATIVE_DRIVING_TIME_MINUTES", DoubleType(), True), StructField("CUMULATIVE_STOPPING_TIME_MINUTES", DoubleType(), True),
    StructField("CUMULATIVE_LINK_LENGTH", DoubleType(), True), StructField("SIGUNGU_ID", StringType(), True),
    StructField("AREA_ID", IntegerType(), True), StructField("START_TIME_MINUTES", DoubleType(), True),
])


# --- 2. Sparkì„ ì´ìš©í•œ ë°ì´í„° ë¶„ì„ ---
def run_spark_analysis():
    """Sparkì„ ì‚¬ìš©í•˜ì—¬ ì—°ê°„/ì¼ë³„ í†µí–‰ëŸ‰, ì •ì§€ ìœ í˜•, ì¼ì¼ ì£¼í–‰ê±°ë¦¬ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("\n--- 2. Spark ë¶„ì„ ì‹œì‘ (ì›”ë³„ ìˆœì°¨ ì²˜ë¦¬) ---")
    
    spark = SparkSession.builder.appName("CombinedTrafficAnalysis_Monthly").config("spark.driver.memory", "32g").config("spark.driver.host", "127.0.0.1").getOrCreate()
    
    yearly_link_volume_acc, daily_total_volume_acc, stopping_df_acc, daily_distance_acc = None, None, None, None
    all_driving_segments_acc = None
    
    for month in range(1, 13):
        year, month_str = 2020, f"2020-{month:02d}"
        monthly_path = os.path.join(PARQUET_BASE_PATH, month_str)
        print(f"\n--- {month_str} ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ ---")
        
        try:
            df_month = spark.read.schema(processed_schema).parquet(monthly_path)
            df_month.cache()
            count = df_month.count()
            if count == 0:
                print(f"  - {month_str} í´ë”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤."); df_month.unpersist(); continue
            print(f"  - {month_str} ë°ì´í„° {count:,} ê±´ ë¡œë“œ ì™„ë£Œ.")

            monthly_link_volume = df_month.groupBy("LINK_ID").count()
            if yearly_link_volume_acc is None: yearly_link_volume_acc = monthly_link_volume
            else: yearly_link_volume_acc = yearly_link_volume_acc.join(monthly_link_volume, "LINK_ID", "full_outer").select(F.col("LINK_ID"), (F.coalesce(yearly_link_volume_acc["count"], F.lit(0)) + F.coalesce(monthly_link_volume["count"], F.lit(0))).alias("count"))

            monthly_daily_volume = df_month.withColumn("date", F.to_date("DATETIME")).groupBy("date").count().withColumnRenamed("count", "Daily_Volume")
            if daily_total_volume_acc is None: daily_total_volume_acc = monthly_daily_volume
            else: daily_total_volume_acc = daily_total_volume_acc.unionByName(monthly_daily_volume)

            monthly_stopping_df = df_month.filter(F.col("STOPPING_TIME") > 0)
            if stopping_df_acc is None: stopping_df_acc = monthly_stopping_df
            else: stopping_df_acc = stopping_df_acc.unionByName(monthly_stopping_df)
            
            monthly_daily_distance = df_month.withColumn("date", F.to_date("DATETIME")).groupBy("OBU_ID", "date").agg(F.sum("LINK_LENGTH").alias("daily_distance_km"))
            if daily_distance_acc is None: daily_distance_acc = monthly_daily_distance
            else: daily_distance_acc = daily_distance_acc.unionByName(monthly_daily_distance)

            if all_driving_segments_acc is None: all_driving_segments_acc = df_month
            else: all_driving_segments_acc = all_driving_segments_acc.unionByName(df_month)
            
            df_month.unpersist()
        except Exception as e:
            print(f"  - ğŸš¨ {month_str} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ë‹¤ìŒ ì›”ë¡œ ì§„í–‰í•©ë‹ˆë‹¤."); continue
    
    print("\n--- ëª¨ë“  ì›” ë°ì´í„° ëˆ„ì  ì™„ë£Œ. ìµœì¢… ì €ì¥ ë° ë¶„ì„ ì‹œì‘ ---")
    try:
        if all_driving_segments_acc:
             all_driving_segments_acc.coalesce(1).write.mode("overwrite").option("header", "true").csv(ALL_DRIVING_SEGMENTS_CSV_PATH)
             print("  - âœ… ì „ì²´ ì£¼í–‰ ê¸°ë¡ ë°ì´í„° CSV ì €ì¥ ì™„ë£Œ.")

        if yearly_link_volume_acc:
            yearly_link_volume_acc.withColumnRenamed("count", "Yearly_Volume").withColumn("Year", F.lit(2020)).coalesce(1).write.mode("overwrite").option("header", "true").csv(YEARLY_LINK_VOLUME_CSV_PATH)
            print("  - âœ… ìµœì¢… ì—°ê°„ ë§í¬ë³„ í†µí–‰ëŸ‰ CSV ì €ì¥ ì™„ë£Œ.")
        
        if daily_total_volume_acc:
            daily_total_volume_acc.withColumn("Day_of_Week", F.date_format(F.col("date"), "E")).coalesce(1).write.mode("overwrite").option("header", "true").csv(DAILY_TOTAL_VOLUME_WITH_HOLIDAYS_CSV_PATH)
            holidays_2020 = ['2020-01-01', '2020-01-24', '2020-01-25', '2020-01-26', '2020-01-27', '2020-03-01', '2020-04-15', '2020-04-30', '2020-05-01', '2020-05-05', '2020-06-06', '2020-08-15', '2020-08-17', '2020-09-30', '2020-10-01', '2020-10-02', '2020-10-03', '2020-10-09', '2020-12-25']
            daily_total_volume_acc.filter(~F.col("date").cast("string").isin(holidays_2020)).withColumn("Day_of_Week", F.date_format(F.col("date"), "E")).coalesce(1).write.mode("overwrite").option("header", "true").csv(DAILY_TOTAL_VOLUME_WITHOUT_HOLIDAYS_CSV_PATH)
            print("  - âœ… ìµœì¢… ì¼ë³„ í†µí–‰ëŸ‰(ê³µíœ´ì¼ í¬í•¨/ì œì™¸) CSV 2ì¢… ì €ì¥ ì™„ë£Œ.")
            
        if daily_distance_acc:
            daily_distance_acc.coalesce(1).write.mode("overwrite").option("header", "true").csv(DAILY_DISTANCE_CSV_PATH)
            print("  - âœ… ìµœì¢… OBUë³„ ì¼ì¼ ì£¼í–‰ê±°ë¦¬ CSV ì €ì¥ ì™„ë£Œ.")

        if stopping_df_acc and stopping_df_acc.count() > 0:
            stopping_df_final = stopping_df_acc.dropna(subset=["STOPPING_TIME"]).coalesce(1)
            stopping_df_final.persist()
            try:
                print(f"  - âœ… ìµœì¢… êµ°ì§‘ ë¶„ì„ ëŒ€ìƒ ë°ì´í„° {stopping_df_final.count():,} ê±´ ì¤€ë¹„ ì™„ë£Œ.")
                pipeline = Pipeline(stages=[VectorAssembler(inputCols=["STOPPING_TIME"], outputCol="features"), KMeans(k=4, seed=1)])
                model = pipeline.fit(stopping_df_final)
                clustered_df = model.transform(stopping_df_final)
                cluster_summary = clustered_df.groupBy("prediction").agg(F.mean("STOPPING_TIME").alias("avg_stop_time")).orderBy("avg_stop_time")
                ranked_clusters = cluster_summary.withColumn("rank", F.row_number().over(Window.orderBy("avg_stop_time")) - 1).collect()
                label_map_dict = {row['prediction']: f"{stop_type}({row['rank']})" for row, stop_type in zip(ranked_clusters, ["Short Stop", "Work Stop", "End-of-Day Stop", "Long Stop"])}
                flattened_map = list(itertools.chain.from_iterable(label_map_dict.items()))
                mapping_expr = F.create_map(*[F.lit(x) for x in flattened_map])
                final_stops_df = clustered_df.withColumn("stop_type_label", mapping_expr[F.col("prediction")])
                
                final_stops_df.drop("features", "prediction") \
                    .coalesce(1).write.mode("overwrite").option("header", "true").csv(CLUSTERED_STOPS_CSV_PATH)
                    
                print("  - âœ… ì •ì§€ ì‹œê°„ êµ°ì§‘ ë¶„ì„ ë°ì´í„° CSV ì €ì¥ ì™„ë£Œ.")
            finally: 
                stopping_df_final.unpersist()
        else: 
            print("  - âš ï¸ ìµœì¢… ëˆ„ì ëœ ì •ì§€ ë°ì´í„°ê°€ ì—†ì–´ K-Means êµ°ì§‘ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    finally:
        spark.stop()
        print("\n--- Spark ì„¸ì…˜ ì¢…ë£Œ ---")


# --- 3. ì‹œê°í™” í•¨ìˆ˜ ---
def create_traffic_heatmap():
    """ì—¬ëŸ¬ ì¡°ê±´ì— ë”°ë¼ ë‹¤ì–‘í•œ ë²„ì „ì˜ Heatmapì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print("\n--- 3a. ë„ë¡œ ë„¤íŠ¸ì›Œí¬ Heatmap ìƒì„± ì‹œì‘ ---")
    try:
        csv_folder_path = YEARLY_LINK_VOLUME_CSV_PATH
        if not os.path.exists(csv_folder_path):
            print(f"ê²½ê³ : Heatmap CSV í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {csv_folder_path}. ê±´ë„ˆëœë‹ˆë‹¤."); return
        csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]
        if not csv_files: raise FileNotFoundError(f"CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_folder_path}")
        csv_file_path = os.path.join(csv_folder_path, csv_files[0])
        gdf_roads, df_volume = gpd.read_file(SHAPEFILE_PATH), pd.read_csv(csv_file_path)
        
        gdf_roads['link_id'] = gdf_roads['link_id'].astype(str)
        df_volume.rename(columns={'LINK_ID': 'link_id'}, inplace=True)
        df_volume['link_id'] = df_volume['link_id'].astype(str)
        merged_gdf = gdf_roads.merge(df_volume, on='link_id', how='inner')
        if merged_gdf.crs != TARGET_CRS: merged_gdf = merged_gdf.to_crs(TARGET_CRS)
        
        map_data = merged_gdf[merged_gdf['Yearly_Volume'] > 0].copy()
        map_data['quantile_all'] = pd.qcut(map_data['Yearly_Volume'], q=5, labels=False, duplicates='drop')
        color_dict = {0: 'blue', 1: 'green', 2: 'yellow', 3: 'orange', 4: 'red'}
        print("  - âœ… ë°ì´í„° ì¤€ë¹„ ë° ì „ì²´ 5ë¶„ìœ„ìˆ˜ ê³„ì‚° ì™„ë£Œ.")

        print("\n  --- ë“±ê¸‰ë³„ ëˆ„ì  ì§€ë„ ìƒì„± ì¤‘ ---")
        style_function_all = lambda x: {'color': color_dict.get(x['properties']['quantile_all'], 'grey'), 'weight': 2.5, 'opacity': 0.8}
        for i in range(4, -1, -1):
            filtered_data = map_data[map_data['quantile_all'] >= i]
            percentage = (5 - i) * 20
            filename = os.path.join(HEATMAP_VIS_DIR, f"heatmap_top{percentage}pct.html")
            if i == 0: filename = os.path.join(HEATMAP_VIS_DIR, "heatmap_all.html")
            print(f"    - ìƒìœ„ {percentage}% ì§€ë„ ìƒì„± ì¤‘... ({len(filtered_data):,}ê°œ ë§í¬)")
            m = folium.Map(location=[36.5, 127.8], zoom_start=7, tiles='CartoDB positron', control_scale=True)
            folium.GeoJson(filtered_data, style_function=style_function_all, tooltip=folium.GeoJsonTooltip(fields=['link_id', 'road_name', 'Yearly_Volume', 'quantile_all'], aliases=['Link ID:', 'Road Name:', 'Yearly Volume:', 'Overall Quantile:'])).add_to(m)
            m.save(filename)

        print("\n  --- í†µí–‰ëŸ‰ ìƒìœ„ 25% ì§€ë„ ìƒì„± ì¤‘ ---")
        top_25_threshold = map_data['Yearly_Volume'].quantile(0.75)
        top_25_data = map_data[map_data['Yearly_Volume'] >= top_25_threshold].copy()
        top_25_data['quantile_top25'] = pd.qcut(top_25_data['Yearly_Volume'], q=5, labels=False, duplicates='drop')
        style_function_top25 = lambda x: {'color': color_dict.get(x['properties']['quantile_top25'], 'grey'),'weight': 2.5, 'opacity': 0.8}
        filename = os.path.join(HEATMAP_VIS_DIR, "heatmap_top25vol.html")
        print(f"    - ìƒìœ„ 25% ì§€ë„ ìƒì„± ì¤‘... ({len(top_25_data):,}ê°œ ë§í¬)")
        m_top25 = folium.Map(location=[36.5, 127.8], zoom_start=7, tiles='CartoDB positron', control_scale=True)
        folium.GeoJson(top_25_data, style_function=style_function_top25, tooltip=folium.GeoJsonTooltip(fields=['link_id', 'road_name', 'Yearly_Volume', 'quantile_top25'], aliases=['Link ID:', 'Road Name:', 'Yearly Volume:', 'Top 25% Quantile:'])).add_to(m_top25)
        m_top25.save(filename)
        print(f"  - âœ… ëª¨ë“  Heatmap ì €ì¥ ì™„ë£Œ.")

    except Exception as e:
        print(f"ì˜¤ë¥˜: Heatmap ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")

def create_daily_volume_boxplot():
    """ê³µíœ´ì¼ í¬í•¨/ì œì™¸ ë‘ ê°€ì§€ ë²„ì „ì˜ Boxplotì„ ëª¨ë‘ ìƒì„±í•©ë‹ˆë‹¤."""
    print("\n--- 3b. ìš”ì¼ë³„ í†µí–‰ëŸ‰ Boxplot ìƒì„± ì‹œì‘ ---")
    plot_configs = [
        {"title": "Daily Volume Distribution by Day of Week (Holidays INCLUDED)", "csv_path": DAILY_TOTAL_VOLUME_WITH_HOLIDAYS_CSV_PATH, "output_png": BOXPLOT_WITH_HOLIDAYS_PNG},
        {"title": "Daily Volume Distribution by Day of Week (Holidays EXCLUDED)", "csv_path": DAILY_TOTAL_VOLUME_WITHOUT_HOLIDAYS_CSV_PATH, "output_png": BOXPLOT_WITHOUT_HOLIDAYS_PNG}
    ]
    for config in plot_configs:
        try:
            print(f"\n  - '{config['title']}' ê·¸ë˜í”„ ìƒì„± ì¤‘...")
            csv_folder_path = config["csv_path"]
            if not os.path.exists(csv_folder_path):
                print(f"    ê²½ê³ : CSV í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {csv_folder_path}. ê±´ë„ˆëœë‹ˆë‹¤."); continue
            csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]
            if not csv_files: raise FileNotFoundError(f"CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_folder_path}")
            csv_file_path = os.path.join(csv_folder_path, csv_files[0])
            df = pd.read_csv(csv_file_path)
            day_order = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
            df['Day_of_Week'] = pd.Categorical(df['Day_of_Week'], categories=day_order, ordered=True)
            
            fig, ax = plt.subplots(figsize=(16, 10))
            sns.boxplot(x='Day_of_Week', y='Daily_Volume', data=df, ax=ax, palette='viridis', width=0.6, hue='Day_of_Week', legend=False)
            max_y_for_text = 0
            for i, day in enumerate(day_order):
                subset = df[df['Day_of_Week'] == day]
                if subset.empty: continue
                min_v, max_v, avg_v, n_v = subset['Daily_Volume'].min(), subset['Daily_Volume'].max(), subset['Daily_Volume'].mean(), len(subset)
                stat_text = f"Min: {min_v:,.0f}\nMax: {max_v:,.0f}\nAvg: {avg_v:,.1f}\nN = {n_v}"
                text_y_position = max_v
                max_y_for_text = max(max_y_for_text, text_y_position)
                ax.text(i, text_y_position * 1.05, stat_text, ha='center', va='bottom', bbox=dict(boxstyle='round,pad=0.5', fc='aliceblue', alpha=0.8), fontsize=10)
            
            if max_y_for_text > 0: ax.set_ylim(top=max_y_for_text * 1.35)
            
            ax.set_title(config["title"], fontsize=20, pad=20)
            ax.set_xlabel('Day of Week', fontsize=14); ax.set_ylabel('Daily Volume', fontsize=14)
            ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: format(int(x), ',')))
            ax.grid(axis='y', linestyle='--', alpha=0.7)
            
            plt.tight_layout(); plt.savefig(config["output_png"], dpi=150); plt.close(fig)
            print(f"    âœ… Boxplot ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {config['output_png']}")
        except Exception as e:
            print(f"    ì˜¤ë¥˜: '{config['title']}' ê·¸ë˜í”„ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")

def create_daily_distance_histogram():
    """ì¼ì¼ ì£¼í–‰ê±°ë¦¬ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ì„ ìƒì„±í•˜ê³ , ìš”ì•½ í†µê³„ëŸ‰ì„ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    print("\n--- 3c. ì¼ì¼ ì£¼í–‰ê±°ë¦¬ íˆìŠ¤í† ê·¸ë¨ ë° í†µê³„ ë¶„ì„ ì‹œì‘ ---")
    try:
        csv_folder_path = DAILY_DISTANCE_CSV_PATH
        if not os.path.exists(csv_folder_path):
            print(f"ê²½ê³ : ì¼ì¼ ì£¼í–‰ê±°ë¦¬ CSV í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {csv_folder_path}. ê±´ë„ˆëœë‹ˆë‹¤."); return
        csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]
        if not csv_files: raise FileNotFoundError(f"CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_folder_path}")
        csv_file_path = os.path.join(csv_folder_path, csv_files[0])
        df = pd.read_csv(csv_file_path)

        daily_distance_stats = df['daily_distance_km'].describe()
        daily_distance_stats.to_csv(DAILY_DISTANCE_STATS_CSV_PATH)
        print(f"  - âœ… ì¼ì¼ ì£¼í–‰ê±°ë¦¬ ìš”ì•½ í†µê³„ ì €ì¥ ì™„ë£Œ: {os.path.basename(DAILY_DISTANCE_STATS_CSV_PATH)}")
        
        plt.figure(figsize=(12, 7))
        upper_bound = df['daily_distance_km'].quantile(0.99)
        sns.histplot(df[df['daily_distance_km'] <= upper_bound]['daily_distance_km'], bins=50, kde=True)
        avg_dist = df['daily_distance_km'].mean()
        plt.axvline(avg_dist, color='red', linestyle='--', label=f'Average: {avg_dist:.1f} km')
        plt.title('Distribution of Daily Driving Distance per Truck (up to 99th percentile)', fontsize=16)
        plt.xlabel('Daily Driving Distance (km)', fontsize=12); plt.ylabel('Number of Daily Trips', fontsize=12)
        plt.legend(); plt.grid(axis='y', linestyle='--', alpha=0.7)
        plt.tight_layout(); plt.savefig(DAILY_DISTANCE_HISTOGRAM_PNG, dpi=150); plt.close()
        print(f"  - âœ… ì¼ì¼ ì£¼í–‰ê±°ë¦¬ íˆìŠ¤í† ê·¸ë¨ ì €ì¥ ì™„ë£Œ.")
    except Exception as e:
        print(f"ì˜¤ë¥˜: ì¼ì¼ ì£¼í–‰ê±°ë¦¬ íˆìŠ¤í† ê·¸ë¨ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")

def create_stop_duration_scatter_plot():
    """ì •ì§€ ìœ í˜•ë³„ Scatter Plotì˜ ë²”ë¡€ì— ì‹œê°„ ë²”ìœ„ì™€ ê°œìˆ˜ë¥¼ ì¶”ê°€í•˜ì—¬ 2ì¢… ìƒì„±í•©ë‹ˆë‹¤."""
    print("\n--- 3d. ì •ì§€ ìœ í˜•ë³„ Scatter Plot ìƒì„± ì‹œì‘ ---")
    try:
        csv_folder_path = CLUSTERED_STOPS_CSV_PATH
        if not os.path.exists(csv_folder_path):
            print(f"ê²½ê³ : Scatter Plotìš© CSV í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤."); return
        csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]
        if not csv_files: raise FileNotFoundError(f"CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_folder_path}")
        csv_file_path = os.path.join(csv_folder_path, csv_files[0])
        df = pd.read_csv(csv_file_path)
        
        if df.empty: print(f"ê²½ê³ : ì •ì§€ ë°ì´í„°ê°€ ë¹„ì–´ìˆì–´ Scatter Plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); return
            
        total_counts = df['stop_type_label'].value_counts()
        time_ranges = df.groupby('stop_type_label')['STOPPING_TIME'].agg(['min', 'max'])
        
        sampling_frac = 0.001
        sampled_df = df.sample(frac=sampling_frac, random_state=42).reset_index(drop=True)
        if sampled_df.empty: print(f"ê²½ê³ : ìƒ˜í”Œë§ëœ ë°ì´í„°ê°€ ì—†ì–´ Scatter Plotì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); return
        
        sampled_counts = sampled_df['stop_type_label'].value_counts()
        sampled_df['Stop_Duration_Hours'] = sampled_df['STOPPING_TIME'] / 60
        type_order_original = sorted(sampled_df['stop_type_label'].unique(), key=lambda x: int(x[-2]))

        def create_legend_label_sampled(label):
            count = sampled_counts.get(label, 0)
            time_range = time_ranges.loc[label]
            return f"{label} (n={count}) (Range: {time_range['min']:.1f}-{time_range['max']:.1f} min)"

        sampled_df['legend_label_sampled'] = sampled_df['stop_type_label'].apply(create_legend_label_sampled)
        hue_order_sampled = [create_legend_label_sampled(x) for x in type_order_original]

        plt.figure(figsize=(18, 9))
        sns.scatterplot(data=sampled_df, x=sampled_df.index, y='Stop_Duration_Hours', hue='legend_label_sampled', style='legend_label_sampled', hue_order=hue_order_sampled, style_order=hue_order_sampled, s=50)
        plt.title(f'Stop Duration vs. Sequence Index by Type (Sampled {sampling_frac:.1%}) - Sampled Counts')
        plt.xlabel('Stop Sequence Index (Sampled)'); plt.ylabel('Stop Duration (Hours)')
        plt.legend(title='Stop Type (n = count in sample)'); plt.grid(True, linestyle='--', alpha=0.6)
        plt.tight_layout()
        filename_sampled = SCATTER_PLOT_PNG.replace('.png', '_sampled_counts.png')
        plt.savefig(filename_sampled, dpi=150); plt.close()
        print(f"  - âœ… Scatter Plot (ìƒ˜í”Œë§ ê°œìˆ˜ ë²„ì „) ì €ì¥ ì™„ë£Œ.")

        def create_legend_label_total(label):
            count = total_counts.get(label, 0)
            time_range = time_ranges.loc[label]
            return f"{label} (N={count}) (Range: {time_range['min']:.1f}-{time_range['max']:.1f} min)"

        sampled_df['legend_label_total'] = sampled_df['stop_type_label'].apply(create_legend_label_total)
        hue_order_total = [create_legend_label_total(x) for x in type_order_original]

        plt.figure(figsize=(18, 9))
        sns.scatterplot(data=sampled_df, x=sampled_df.index, y='Stop_Duration_Hours', hue='legend_label_total', style='legend_label_total', hue_order=hue_order_total, style_order=hue_order_total, s=50)
        plt.title(f'Stop Duration vs. Sequence Index by Type (Sampled {sampling_frac:.1%}) - Total Counts')
        plt.xlabel('Stop Sequence Index (Sampled)'); plt.ylabel('Stop Duration (Hours)')
        plt.legend(title='Stop Type (N = count in total data)'); plt.grid(True, linestyle='--', alpha=0.6)
        plt.tight_layout()
        filename_total = SCATTER_PLOT_PNG.replace('.png', '_total_counts.png')
        plt.savefig(filename_total, dpi=150); plt.close()
        print(f"  - âœ… Scatter Plot (ì „ì²´ ê°œìˆ˜ ë²„ì „) ì €ì¥ ì™„ë£Œ.")
    except Exception as e:
        print(f"ì˜¤ë¥˜: Scatter Plot ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")
        
def create_stop_location_maps():
    """ì •ì§€ ìœ í˜•ë³„ ìœ„ì¹˜ ì§€ë„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print("\n--- 3e. ì •ì§€ ìœ í˜•ë³„ ìœ„ì¹˜ ì§€ë„ ìƒì„± ì‹œì‘ ---")
    try:
        csv_folder_path = CLUSTERED_STOPS_CSV_PATH
        if not os.path.exists(csv_folder_path):
            print(f"ê²½ê³ : ì •ì§€ ìœ„ì¹˜ ì§€ë„ë¥¼ ìœ„í•œ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤."); return
        csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]
        if not csv_files: raise FileNotFoundError(f"CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_folder_path}")
        csv_file_path = os.path.join(csv_folder_path, csv_files[0])
        df_stops = pd.read_csv(csv_file_path)

        gdf_roads = gpd.read_file(SHAPEFILE_PATH, columns=['link_id', 'geometry'])
        gdf_roads.rename(columns={'link_id': 'LINK_ID'}, inplace=True)

        df_stops['LINK_ID'] = df_stops['LINK_ID'].astype(str)
        gdf_roads['LINK_ID'] = gdf_roads['LINK_ID'].astype(str)
        
        stop_counts = df_stops.groupby(['LINK_ID', 'stop_type_label']).size().reset_index(name='stop_count')
        merged_gdf = gdf_roads.merge(stop_counts, on='LINK_ID', how='inner')
        if merged_gdf.crs != TARGET_CRS: merged_gdf = merged_gdf.to_crs(TARGET_CRS)
        
        merged_gdf['lon'] = merged_gdf.geometry.centroid.x
        merged_gdf['lat'] = merged_gdf.geometry.centroid.y
        
        stop_types = sorted(merged_gdf['stop_type_label'].unique(), key=lambda x: int(x[-2]))
        type_colors = {'Short Stop(0)': 'blue', 'Work Stop(1)': 'green', 'End-of-Day Stop(2)': 'purple', 'Long Stop(3)': 'red'}

        for stop_type in stop_types:
            type_data = merged_gdf[merged_gdf['stop_type_label'] == stop_type]
            if type_data.empty: continue
            
            m = folium.Map(location=[36.5, 127.8], zoom_start=7, tiles='CartoDB positron')
            for _, row in type_data.iterrows():
                folium.CircleMarker(
                    location=[row['lat'], row['lon']], radius=max(2, min(10, row['stop_count'] / 5)),
                    color=type_colors.get(stop_type, 'gray'), fill=True, fill_color=type_colors.get(stop_type, 'gray'),
                    tooltip=f"Link: {row['LINK_ID']}<br>Type: {row['stop_type_label']}<br>Count: {row['stop_count']}"
                ).add_to(m)
            
            filename = os.path.join(STOP_LOCATION_MAP_DIR, f"stop_locations_{stop_type.replace('(', '').replace(')', '')}.html")
            m.save(filename)
            print(f"  - âœ… {stop_type} ì •ì§€ ìœ„ì¹˜ ì§€ë„ ì €ì¥ ì™„ë£Œ.")
            
        m_all = folium.Map(location=[36.5, 127.8], zoom_start=7, tiles='CartoDB positron')
        for stop_type in stop_types:
            type_data = merged_gdf[merged_gdf['stop_type_label'] == stop_type]
            fg = folium.FeatureGroup(name=stop_type)
            for _, row in type_data.iterrows():
                folium.CircleMarker(
                    location=[row['lat'], row['lon']], radius=max(2, min(10, row['stop_count'] / 5)),
                    color=type_colors.get(stop_type, 'gray'), fill=True, fill_color=type_colors.get(stop_type, 'gray'),
                    tooltip=f"Link: {row['LINK_ID']}<br>Type: {row['stop_type_label']}<br>Count: {row['stop_count']}"
                ).add_to(fg)
            fg.add_to(m_all)
        
        folium.LayerControl().add_to(m_all)
        filename_all = os.path.join(STOP_LOCATION_MAP_DIR, "stop_locations_all_types.html")
        m_all.save(filename_all)
        print("  - âœ… ëª¨ë“  ìœ í˜• ì¢…í•© ì§€ë„ ì €ì¥ ì™„ë£Œ.")

    except Exception as e:
        print(f"ì˜¤ë¥˜: ì •ì§€ ìœ„ì¹˜ ì§€ë„ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")

# --- 4. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡ ---
if __name__ == "__main__":
    total_start_time = time.time()
    
    print("\nâœ¨ ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    run_spark_analysis()
    
    create_traffic_heatmap()
    create_daily_volume_boxplot()
    create_daily_distance_histogram()
    create_stop_duration_scatter_plot()
    create_stop_location_maps()
    
    print(f"\nâœ¨ ëª¨ë“  ì‘ì—… ì™„ë£Œ. ì´ ì†Œìš” ì‹œê°„: {(time.time() - total_start_time) / 60:.2f} ë¶„.")